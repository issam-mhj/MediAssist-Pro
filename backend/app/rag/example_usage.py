"""
Script d'exemple pour le prÃ©traitement et le chunking de documents PDF.
DÃ©montre l'utilisation du DocumentProcessor et DocumentChunker.
"""

from document_processor import DocumentProcessor, load_documents
from chunking import DocumentChunker
import json


def main():
    print("\n" + "="*70)
    print("DÃ‰MONSTRATION COMPLÃˆTE : PRÃ‰TRAITEMENT ET CHUNKING")
    print("="*70 + "\n")
    
    # ========================================================================
    # Ã‰TAPE 1: Chargement et extraction du PDF
    # ========================================================================
    print("ğŸ“š Ã‰TAPE 1: Chargement du document PDF")
    print("-" * 70)
    
    processor = DocumentProcessor()
    
    # Afficher les mÃ©tadonnÃ©es du document
    print("\nğŸ“‹ MÃ©tadonnÃ©es du document:")
    metadata = processor.extract_metadata()
    for key, value in metadata.items():
        print(f"  â€¢ {key}: {value}")
    
    # Charger tous les documents
    print("\nğŸ“¥ Extraction du contenu...")
    documents = processor.load_pdf()
    
    print(f"\nâœ… {len(documents)} pages extraites")
    print(f"   Total de caractÃ¨res: {sum(len(doc.page_content) for doc in documents):,}")
    
    # Exemple d'extraction d'une page spÃ©cifique
    print("\nğŸ“„ Exemple: Extraction de la page 1")
    page_1_text = processor.get_page_text(1)
    print(f"   Longueur: {len(page_1_text)} caractÃ¨res")
    print(f"   AperÃ§u: {page_1_text[:150]}...")
    
    # ========================================================================
    # Ã‰TAPE 2: Chunking par caractÃ¨res (mÃ©thode fixe)
    # ========================================================================
    print("\n\nğŸ“ Ã‰TAPE 2: Chunking par caractÃ¨res")
    print("-" * 70)
    
    chunker = DocumentChunker(chunk_size=1000, chunk_overlap=200)
    char_chunks = chunker.create_character_chunks(documents)
    
    # Statistiques
    stats = chunker.get_chunk_stats(char_chunks)
    print("\nğŸ“Š Statistiques:")
    print(f"   â€¢ Nombre de chunks: {stats['total_chunks']}")
    print(f"   â€¢ Taille minimale: {stats['min_length']} caractÃ¨res")
    print(f"   â€¢ Taille maximale: {stats['max_length']} caractÃ¨res")
    print(f"   â€¢ Taille moyenne: {stats['avg_length']:.0f} caractÃ¨res")
    print(f"   â€¢ Total: {stats['total_chars']:,} caractÃ¨res")
    
    # AperÃ§u d'un chunk
    print("\nğŸ“„ AperÃ§u du chunk #10:")
    if len(char_chunks) >= 10:
        chunk = char_chunks[9]
        print(f"   â€¢ Page source: {chunk.metadata['page']}")
        print(f"   â€¢ Longueur: {len(chunk.page_content)} caractÃ¨res")
        print(f"   â€¢ Contenu: {chunk.page_content[:200]}...")
    
    # ========================================================================
    # Ã‰TAPE 3: Chunking sÃ©mantique
    # ========================================================================
    print("\n\nğŸ§  Ã‰TAPE 3: Chunking sÃ©mantique")
    print("-" * 70)
    print("âš ï¸  Note: Le chunking sÃ©mantique nÃ©cessite le tÃ©lÃ©chargement")
    print("   d'un modÃ¨le d'embeddings (peut prendre quelques minutes)")
    print("   au premier lancement.\n")
    
    # Utilisation d'un sous-ensemble pour la dÃ©mo
    sample_docs = documents[:5]  # PremiÃ¨res 5 pages seulement
    print(f"ğŸ“Œ Utilisation de {len(sample_docs)} pages pour la dÃ©mo")
    
    try:
        semantic_chunks = chunker.create_semantic_chunks(sample_docs)
        
        # Statistiques
        sem_stats = chunker.get_chunk_stats(semantic_chunks)
        print(f"\nğŸ“Š Statistiques sÃ©mantiques:")
        print(f"   â€¢ Nombre de chunks: {sem_stats['total_chunks']}")
        print(f"   â€¢ Taille moyenne: {sem_stats['avg_length']:.0f} caractÃ¨res")
        print(f"   â€¢ Min/Max: {sem_stats['min_length']} / {sem_stats['max_length']}")
        
        # AperÃ§u
        print("\nğŸ“„ AperÃ§u du premier chunk sÃ©mantique:")
        if semantic_chunks:
            chunk = semantic_chunks[0]
            print(f"   â€¢ Page source: {chunk.metadata['page']}")
            print(f"   â€¢ Longueur: {len(chunk.page_content)} caractÃ¨res")
            print(f"   â€¢ Contenu: {chunk.page_content[:200]}...")
    
    except Exception as e:
        print(f"\nâš ï¸  Chunking sÃ©mantique ignorÃ©: {str(e)}")
        print("   (Vous pouvez installer les dÃ©pendances si nÃ©cessaire)")
    
    # ========================================================================
    # Ã‰TAPE 4: Sauvegarde des chunks (optionnel)
    # ========================================================================
    print("\n\nğŸ’¾ Ã‰TAPE 4: Exemple de sauvegarde")
    print("-" * 70)
    
    # PrÃ©parer les donnÃ©es pour la sauvegarde
    chunks_data = []
    for i, chunk in enumerate(char_chunks[:5], 1):  # Premiers 5 chunks
        chunks_data.append({
            "chunk_id": chunk.metadata.get("chunk_id"),
            "page": chunk.metadata.get("page"),
            "content": chunk.page_content[:100] + "...",  # TronquÃ© pour la dÃ©mo
            "char_count": len(chunk.page_content)
        })
    
    print("\nğŸ“¦ Exemple de structure des chunks (5 premiers):")
    print(json.dumps(chunks_data, indent=2, ensure_ascii=False)[:500] + "...")
    
    # ========================================================================
    # RÃ‰SUMÃ‰ FINAL
    # ========================================================================
    print("\n\n" + "="*70)
    print("âœ… RÃ‰SUMÃ‰")
    print("="*70)
    print(f"ğŸ“„ Document traitÃ©: {metadata.get('file_name', 'N/A')}")
    print(f"ğŸ“Š Pages extraites: {len(documents)}")
    print(f"ğŸ“ Chunks crÃ©Ã©s (caractÃ¨res): {len(char_chunks)}")
    print(f"ğŸ’¾ Taille totale: {sum(len(c.page_content) for c in char_chunks):,} caractÃ¨res")
    print("\nğŸ‰ PrÃ©traitement et chunking terminÃ©s avec succÃ¨s!")
    print("="*70 + "\n")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  OpÃ©ration interrompue par l'utilisateur")
    except Exception as e:
        print(f"\n\nâŒ Erreur: {str(e)}")
        import traceback
        traceback.print_exc()
