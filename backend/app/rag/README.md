# Module RAG - PrÃ©traitement et Chunking de Documents

Ce module fournit des outils complets pour le prÃ©traitement et le chunking de documents PDF dans le cadre du systÃ¨me RAG (Retrieval-Augmented Generation) de MediAssist-Pro.

## ğŸ“‹ Table des matiÃ¨res

- [Vue d'ensemble](#vue-densemble)
- [Architecture](#architecture)
- [Modules](#modules)
- [Installation](#installation)
- [Utilisation](#utilisation)
- [Exemples](#exemples)
- [Configuration](#configuration)

## ğŸ¯ Vue d'ensemble

Le systÃ¨me de prÃ©traitement et chunking permet de :

1. **Extraire** le texte de documents PDF techniques (manuels de maintenance)
2. **Nettoyer** et normaliser le texte extrait
3. **DÃ©couper** le contenu en chunks optimisÃ©s pour la recherche sÃ©mantique
4. **Enrichir** avec des mÃ©tadonnÃ©es pour un meilleur tracking

## ğŸ—ï¸ Architecture

```
backend/app/rag/
â”œâ”€â”€ document_processor.py   # Extraction et prÃ©traitement PDF
â”œâ”€â”€ chunking.py             # StratÃ©gies de chunking
â”œâ”€â”€ embeddings.py           # GÃ©nÃ©ration d'embeddings
â”œâ”€â”€ vector_store.py         # Stockage vectoriel (ChromaDB)
â”œâ”€â”€ retriever.py            # Recherche et rÃ©cupÃ©ration
â”œâ”€â”€ generator.py            # GÃ©nÃ©ration de rÃ©ponses
â””â”€â”€ example_usage.py        # Exemples d'utilisation
```

## ğŸ“¦ Modules

### 1. `document_processor.py`

**ResponsabilitÃ©** : Extraction et prÃ©traitement du contenu PDF

#### Classe `DocumentProcessor`

```python
class DocumentProcessor:
    def __init__(self, pdf_path: Optional[str] = None)
    def load_pdf(self) -> List[Document]
    def preprocess_text(self, text: str) -> str
    def extract_metadata(self) -> Dict[str, any]
    def get_page_text(self, page_number: int) -> str
```

**FonctionnalitÃ©s** :
- âœ… Extraction de texte page par page
- âœ… Nettoyage des artefacts PDF (sauts de ligne, espaces)
- âœ… Normalisation des caractÃ¨res spÃ©ciaux
- âœ… Extraction de mÃ©tadonnÃ©es (auteur, pages, taille)
- âœ… Support des documents multi-pages

**Exemple** :
```python
processor = DocumentProcessor()
documents = processor.load_pdf()
# Retourne: List[Document] avec texte et mÃ©tadonnÃ©es
```

### 2. `chunking.py`

**ResponsabilitÃ©** : DÃ©coupage intelligent du contenu

#### Classe `DocumentChunker`

```python
class DocumentChunker:
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200)
    def create_semantic_chunks(documents: List[Document]) -> List[Document]
    def create_character_chunks(documents: List[Document]) -> List[Document]
    def create_hybrid_chunks(documents: List[Document]) -> List[Document]
    def get_chunk_stats(chunks: List[Document]) -> dict
```

#### StratÃ©gies de chunking

##### ğŸ”¤ Chunking par caractÃ¨res (Character-based)
- DÃ©coupe fixe basÃ©e sur la taille
- PrÃ©serve les sÃ©parateurs naturels (paragraphes, phrases)
- Chevauchement configurable entre chunks
- **Avantages** : Rapide, prÃ©visible, contrÃ´lable
- **Cas d'usage** : Documents bien structurÃ©s

##### ğŸ§  Chunking sÃ©mantique (Semantic)
- DÃ©coupe basÃ©e sur la similaritÃ© sÃ©mantique
- Utilise des embeddings pour dÃ©tecter les ruptures de contexte
- Chunks de taille variable mais cohÃ©rents sÃ©mantiquement
- **Avantages** : Meilleure cohÃ©rence contextuelle
- **Cas d'usage** : Documents complexes, texte narratif

##### ğŸ”„ Chunking hybride
- Combine les deux approches
- Chunking sÃ©mantique initial + subdivision si nÃ©cessaire
- Ã‰quilibre entre cohÃ©rence et taille

## ğŸš€ Installation

### DÃ©pendances requises

```bash
pip install pypdf langchain langchain-core langchain-community \
            langchain-experimental langchain-text-splitters \
            sentence-transformers
```

Ou via requirements.txt :
```bash
cd backend
pip install -r requirements.txt
```

## ğŸ’» Utilisation

### Utilisation de base

```python
from document_processor import DocumentProcessor
from chunking import DocumentChunker

# 1. Charger et prÃ©traiter le PDF
processor = DocumentProcessor()
documents = processor.load_pdf()

# 2. CrÃ©er des chunks
chunker = DocumentChunker(chunk_size=1000, chunk_overlap=200)
chunks = chunker.create_character_chunks(documents)

print(f"âœ… {len(chunks)} chunks crÃ©Ã©s")
```

### Extraction d'une page spÃ©cifique

```python
processor = DocumentProcessor()

# Extraire la page 5
page_text = processor.get_page_text(5)
print(page_text)
```

### Chunking sÃ©mantique

```python
chunker = DocumentChunker()

# Chunking sÃ©mantique avec seuil percentile
semantic_chunks = chunker.create_semantic_chunks(
    documents,
    breakpoint_threshold_type="percentile"
)
```

### Statistiques sur les chunks

```python
stats = chunker.get_chunk_stats(chunks)
print(f"Nombre de chunks: {stats['total_chunks']}")
print(f"Taille moyenne: {stats['avg_length']}")
print(f"Min/Max: {stats['min_length']}/{stats['max_length']}")
```

## ğŸ“Š Exemples

### Script de dÃ©monstration complet

```bash
cd backend/app/rag
python example_usage.py
```

Ce script dÃ©montre :
1. Chargement et extraction du PDF
2. Affichage des mÃ©tadonnÃ©es
3. Chunking par caractÃ¨res avec statistiques
4. Chunking sÃ©mantique (sur Ã©chantillon)
5. Sauvegarde des rÃ©sultats

### Test unitaires

```bash
# Test du document processor
python document_processor.py

# Test du chunker
python chunking.py
```

## âš™ï¸ Configuration

### ParamÃ¨tres de chunking

```python
chunker = DocumentChunker(
    chunk_size=1000,      # Taille maximale d'un chunk (caractÃ¨res)
    chunk_overlap=200     # Chevauchement entre chunks (caractÃ¨res)
)
```

**Recommandations** :
- **chunk_size** : 500-1500 caractÃ¨res (selon le contexte)
- **chunk_overlap** : 15-20% de chunk_size
- Plus le chunk_size est petit, plus la recherche est prÃ©cise mais fragmentÃ©e
- Plus l'overlap est grand, meilleure est la continuitÃ© mais redondance accrue

### SÃ©parateurs personnalisÃ©s

```python
custom_separators = [
    "\n\n",     # Paragraphes
    "\n",       # Lignes
    ". ",       # Phrases
    ", ",       # Clauses
    " "         # Mots
]

chunks = chunker.create_character_chunks(
    documents,
    separators=custom_separators
)
```

## ğŸ“ˆ Performances

### Document de test
- **Fichier** : `maintenance-des-appareils-de-laboratoire.pdf`
- **Pages** : 57 (55 pages extraites avec contenu)
- **Taille** : ~1.1 MB
- **CaractÃ¨res totaux** : ~185,000

### RÃ©sultats

#### Chunking par caractÃ¨res
- **Chunks crÃ©Ã©s** : 241
- **Taille moyenne** : 868 caractÃ¨res
- **Temps** : < 1 seconde

#### Chunking sÃ©mantique
- **Chunks crÃ©Ã©s** : Variable selon le contenu
- **Taille moyenne** : ~1000 caractÃ¨res
- **Temps** : 2-5 secondes (aprÃ¨s tÃ©lÃ©chargement du modÃ¨le)

## ğŸ” Structure des donnÃ©es

### Document LangChain

```python
Document(
    page_content="Texte du chunk...",
    metadata={
        "source": "maintenance-des-appareils-de-laboratoire.pdf",
        "page": 15,
        "total_pages": 57,
        "file_path": "/path/to/file.pdf",
        "chunk_id": 42,
        "chunk_type": "character",
        "char_count": 950,
        "chunk_size": 1000,
        "chunk_overlap": 200
    }
)
```

## ğŸ› ï¸ Fonctions utilitaires

### `load_documents(pdf_path)`
Fonction standalone pour charger rapidement un PDF

```python
from document_processor import load_documents

docs = load_documents("path/to/file.pdf")
```

## ğŸ“ Notes importantes

### PrÃ©traitement du texte

Le prÃ©traitement effectue :
- âœ… Suppression des espaces multiples
- âœ… Correction des mots coupÃ©s (hyphenation)
- âœ… Normalisation des apostrophes et guillemets
- âœ… Suppression des lignes vides multiples
- âœ… Nettoyage des caractÃ¨res de contrÃ´le

### MÃ©tadonnÃ©es enrichies

Chaque chunk contient :
- Source du document
- NumÃ©ro de page d'origine
- ID unique du chunk
- Type de chunking utilisÃ©
- Taille en caractÃ¨res
- ParamÃ¨tres de chunking

## ğŸš§ AmÃ©liorations futures

- [ ] Support d'autres formats (DOCX, TXT, HTML)
- [ ] DÃ©tection automatique de la structure (titres, sections)
- [ ] Chunking contextuel (par section/chapitre)
- [ ] Filtrage de contenu (tables des matiÃ¨res, index)
- [ ] Cache des embeddings pour performance
- [ ] Support multi-langue amÃ©liorÃ©

## ğŸ› DÃ©pannage

### Erreur : Module 'pypdf' not found
```bash
pip install pypdf
```

### Erreur : Module 'langchain' not found
```bash
pip install langchain langchain-core langchain-community
```

### Chunking sÃ©mantique lent
- Le premier lancement tÃ©lÃ©charge le modÃ¨le d'embeddings (~100MB)
- Les exÃ©cutions suivantes utilisent le cache local
- Utiliser `chunk_size` plus grand pour rÃ©duire le nombre de chunks

### PDF corrompu ou non lisible
- VÃ©rifier que le fichier PDF n'est pas chiffrÃ©
- Essayer de le rÃ©exporter avec un autre outil PDF
- VÃ©rifier les permissions de lecture du fichier

## ğŸ“š Ressources

- [LangChain Documentation](https://python.langchain.com/)
- [pypdf Documentation](https://pypdf.readthedocs.io/)
- [Sentence Transformers](https://www.sbert.net/)
- [ChromaDB Documentation](https://docs.trychroma.com/)

## âœ… Tests validÃ©s

- âœ… Extraction de 57 pages PDF
- âœ… PrÃ©traitement de 185k caractÃ¨res
- âœ… GÃ©nÃ©ration de 241 chunks (caractÃ¨res)
- âœ… GÃ©nÃ©ration de 20 chunks (sÃ©mantique sur Ã©chantillon)
- âœ… MÃ©tadonnÃ©es complÃ¨tes et cohÃ©rentes
- âœ… Statistiques prÃ©cises

---

**Auteur** : MediAssist-Pro Team  
**Date** : FÃ©vrier 2026  
**Version** : 1.0
